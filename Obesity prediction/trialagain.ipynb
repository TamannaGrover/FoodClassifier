{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20758 entries, 0 to 20757\n",
      "Data columns (total 18 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   id                              20758 non-null  int64  \n",
      " 1   Gender                          20758 non-null  object \n",
      " 2   Age                             20758 non-null  float64\n",
      " 3   Height                          20758 non-null  float64\n",
      " 4   Weight                          20758 non-null  float64\n",
      " 5   family_history_with_overweight  20758 non-null  object \n",
      " 6   FAVC                            20758 non-null  object \n",
      " 7   FCVC                            20758 non-null  float64\n",
      " 8   NCP                             20758 non-null  float64\n",
      " 9   CAEC                            20758 non-null  object \n",
      " 10  SMOKE                           20758 non-null  object \n",
      " 11  CH2O                            20758 non-null  float64\n",
      " 12  SCC                             20758 non-null  object \n",
      " 13  FAF                             20758 non-null  float64\n",
      " 14  TUE                             20758 non-null  float64\n",
      " 15  CALC                            20758 non-null  object \n",
      " 16  MTRANS                          20758 non-null  object \n",
      " 17  NObeyesdad                      20758 non-null  object \n",
      "dtypes: float64(8), int64(1), object(9)\n",
      "memory usage: 2.9+ MB\n",
      "None\n",
      "\n",
      "Test Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13840 entries, 0 to 13839\n",
      "Data columns (total 17 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   id                              13840 non-null  int64  \n",
      " 1   Gender                          13840 non-null  object \n",
      " 2   Age                             13840 non-null  float64\n",
      " 3   Height                          13840 non-null  float64\n",
      " 4   Weight                          13840 non-null  float64\n",
      " 5   family_history_with_overweight  13840 non-null  object \n",
      " 6   FAVC                            13840 non-null  object \n",
      " 7   FCVC                            13840 non-null  float64\n",
      " 8   NCP                             13840 non-null  float64\n",
      " 9   CAEC                            13840 non-null  object \n",
      " 10  SMOKE                           13840 non-null  object \n",
      " 11  CH2O                            13840 non-null  float64\n",
      " 12  SCC                             13840 non-null  object \n",
      " 13  FAF                             13840 non-null  float64\n",
      " 14  TUE                             13840 non-null  float64\n",
      " 15  CALC                            13840 non-null  object \n",
      " 16  MTRANS                          13840 non-null  object \n",
      "dtypes: float64(8), int64(1), object(8)\n",
      "memory usage: 1.8+ MB\n",
      "None\n",
      "Best Hyperparameters:\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Validation Accuracy: 0.89\n",
      "\n",
      "Feature Importance:\n",
      "id: 0.0996\n",
      "Gender: 0.1001\n",
      "Age: 0.3472\n",
      "Height: 0.1034\n",
      "Weight: 0.0344\n",
      "family_history_with_overweight: 0.0501\n",
      "FAVC: 0.0439\n",
      "FCVC: 0.0488\n",
      "NCP: 0.0538\n",
      "CAEC: 0.0239\n",
      "SMOKE: 0.0120\n",
      "CH2O: 0.0139\n",
      "SCC: 0.0147\n",
      "FAF: 0.0036\n",
      "TUE: 0.0012\n",
      "CALC: 0.0049\n",
      "MTRANS: 0.0137\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load your data\n",
    "# Assuming 'train.csv' contains your training data and 'test.csv' contains your test data\n",
    "train_df = pd.read_csv(\"D:\\\\Foundathon\\\\Obesity prediction\\\\train.csv\")\n",
    "test_df = pd.read_csv(\"D:\\\\Foundathon\\\\Obesity prediction\\\\test.csv\")\n",
    "\n",
    "# Display basic information about the training data\n",
    "print(\"Training Data Info:\")\n",
    "print(train_df.info())\n",
    "\n",
    "# Display basic information about the test data\n",
    "print(\"\\nTest Data Info:\")\n",
    "print(test_df.info())\n",
    "\n",
    "# Specify features and target variable\n",
    "training_features = [\n",
    "    'id', 'Gender', 'Age', 'Height', 'Weight',\n",
    "    'family_history_with_overweight', 'FAVC', 'FCVC', 'NCP', 'CAEC',\n",
    "    'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE', 'CALC', 'MTRANS'\n",
    "]\n",
    "\n",
    "target_variable = 'NObeyesdad'\n",
    "\n",
    "# Selecting only the relevant features for training\n",
    "X_train = train_df[training_features]\n",
    "y_train = train_df[target_variable]\n",
    "\n",
    "# Data preprocessing\n",
    "# You can add more preprocessing steps based on your data\n",
    "categorical_features = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS']\n",
    "numeric_features = ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']\n",
    "\n",
    "# Selecting only the relevant features for training\n",
    "X_train_categorical = X_train[categorical_features]\n",
    "X_train_numeric = X_train[numeric_features]\n",
    "\n",
    "# One-hot encoding for categorical features\n",
    "X_train_categorical_encoded = pd.get_dummies(X_train_categorical, drop_first=True)\n",
    "\n",
    "# Concatenate one-hot encoded categorical features with numeric features\n",
    "X_train_processed = pd.concat([X_train_numeric, X_train_categorical_encoded], axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_processed)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning (optional)\n",
    "# ... (your existing code)\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [1]\n",
    "}\n",
    "\n",
    "# Create a RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Use GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_params)\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_rf_model = RandomForestClassifier(random_state=42, **best_params)\n",
    "best_rf_model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Model evaluation on the validation set\n",
    "predictions_val = best_rf_model.predict(X_val)\n",
    "accuracy_val = accuracy_score(y_val, predictions_val)\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy_val:.2f}')\n",
    "\n",
    "# Feature importance (optional)\n",
    "feature_importance = best_rf_model.feature_importances_\n",
    "print('\\nFeature Importance:')\n",
    "for feature, importance in zip(training_features, feature_importance):\n",
    "    print(f'{feature}: {importance:.4f}')\n",
    "\n",
    "# Preprocess the test data\n",
    "# Use the same scaler that was fit on the training data\n",
    "X_test_categorical = test_df[categorical_features]\n",
    "X_test_numeric = test_df[numeric_features]\n",
    "\n",
    "# One-hot encoding for categorical features\n",
    "X_test_categorical_encoded = pd.get_dummies(X_test_categorical, drop_first=True)\n",
    "\n",
    "# Make sure column names match those in the training data\n",
    "missing_columns = set(X_train_categorical_encoded.columns) - set(X_test_categorical_encoded.columns)\n",
    "for column in missing_columns:\n",
    "    X_test_categorical_encoded[column] = 0\n",
    "\n",
    "# Concatenate one-hot encoded categorical features with numeric features\n",
    "X_test_processed = pd.concat([X_test_numeric, X_test_categorical_encoded], axis=1)\n",
    "\n",
    "# Reorder the columns to match the order during training\n",
    "# Reorder the columns to match the order during training\n",
    "X_test_processed = X_test_processed[X_train_processed.columns]\n",
    "\n",
    "\n",
    "# Use the same scaler that was fit on the training data\n",
    "X_test_scaled = scaler.transform(X_test_processed)\n",
    "\n",
    "# Model prediction on the test set\n",
    "predictions_test = best_rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Assuming 'output.csv' as the output file for predictions\n",
    "output_df = pd.DataFrame({'id': test_df['id'], 'Predicted_Label': predictions_test})\n",
    "output_df.to_csv('output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_rf_model.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with the best parameters\n",
    "best_rf_model = RandomForestClassifier(random_state=42, **best_params)\n",
    "best_rf_model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Save the trained model to a file\n",
    "joblib.dump(best_rf_model, 'best_rf_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'joblib' from 'sklearn.externals' (c:\\Users\\grove\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\externals\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m joblib  \u001b[38;5;66;03m# For scikit-learn versions <= 0.23.2\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# from joblib import load  # For scikit-learn versions >= 0.24\u001b[39;00m\n\u001b[0;32m      8\u001b[0m app \u001b[38;5;241m=\u001b[39m Flask(\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (c:\\Users\\grove\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\externals\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# app.py\n",
    "from flask import Flask, render_template, request\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals import joblib  # For scikit-learn versions <= 0.23.2\n",
    "# from joblib import load  # For scikit-learn versions >= 0.24\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the trained model and scaler\n",
    "best_rf_model = joblib.load(\"rf_model.joblib\")\n",
    "scaler = joblib.load(\"scaler.joblib\")\n",
    "\n",
    "# Define the route for the home page\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "# Define the route for prediction\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    if request.method == 'POST':\n",
    "        # Get input data from the form\n",
    "        input_data = {\n",
    "            'Gender': request.form['gender'],\n",
    "            'Age': float(request.form['age']),\n",
    "            'Height': float(request.form['height']),\n",
    "            'Weight': float(request.form['weight']),\n",
    "            # ... (add other input fields)\n",
    "        }\n",
    "\n",
    "        # Create a DataFrame from the input data\n",
    "        input_df = pd.DataFrame([input_data])\n",
    "\n",
    "        # Preprocess the input data\n",
    "        input_categorical = input_df[categorical_features]\n",
    "        input_numeric = input_df[numeric_features]\n",
    "        input_categorical_encoded = pd.get_dummies(input_categorical, drop_first=True)\n",
    "        input_processed = pd.concat([input_numeric, input_categorical_encoded], axis=1)\n",
    "        input_scaled = scaler.transform(input_processed)\n",
    "\n",
    "        # Make a prediction using the trained model\n",
    "        prediction = best_rf_model.predict(input_scaled)[0]\n",
    "\n",
    "        # Map the prediction to the corresponding label\n",
    "        labels = {'Insufficient_Weight': 'Underweight', 'Normal_Weight': 'Normal weight', 'Overweight_Level_I': 'Overweight',\n",
    "                  'Overweight_Level_II': 'Obesity Type I', 'Overweight_Level_III': 'Obesity Type II',\n",
    "                  'Overweight_Level_III': 'Obesity Type III'}\n",
    "        predicted_label = labels[prediction]\n",
    "\n",
    "        return render_template('index.html', prediction=predicted_label)\n",
    "\n",
    "# Run the Flask app\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'Gender', 'Age', 'Height', 'Weight',\n",
      "       'family_history_with_overweight', 'FAVC', 'FCVC', 'NCP', 'CAEC',\n",
      "       'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE', 'CALC', 'MTRANS', 'NObeyesdad'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
